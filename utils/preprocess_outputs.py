import torch

def retrieve_eos_token_ids(tokenizer):
    eos_token_ids = [id for token, id in tokenizer.vocab.items() if token.endswith(('.', '!', '?'))]
    return eos_token_ids

def find_sequence_start(tensor, sequence):
    """
    Find the start index of the sequence in the given tensor.
    
    Args:
        tensor (torch.Tensor): Input tensor which could be 1D or 2D.
        sequence (list): Sequence of tokens to find in the tensor.
    
    Returns:
        int or list: Start index of the sequence in the tensor if 1D, 
                     or a list of start indices for each sequence in the tensor if 2D.
    """
    seq_len = len(sequence)
    sequence_tensor = torch.tensor(sequence, device=tensor.device)
    
    if tensor.dim() == 1:
        # Single sequence case
        for i in range(len(tensor) - seq_len + 1):
            if torch.equal(tensor[i:i + seq_len], sequence_tensor):
                return i + seq_len
        return -1  # Sequence not found

    elif tensor.dim() == 2:
        # Batch of sequences case
        start_indices = []
        for seq in tensor:
            found = False
            for i in range(len(seq) - seq_len + 1):
                if torch.equal(seq[i:i + seq_len], sequence_tensor):
                    start_indices.append(i + seq_len)
                    found = True
                    break
            if not found:
                start_indices.append(-1)  # Sequence not found in this sequence
        return start_indices
    
def extract_generated_tokens(tensor, end_of_instruction = [733, 28748, 16289, 28793], pad_token_id = 2):
    """
    Extract the tokens generated by the model after the end_of_instruction sequence.
    
    Args:
        tensor (torch.Tensor): Input tensor which could be size (seq_len) or (batch_size, seq_len)
        end_of_instruction (list): Sequence of tokens marking the end of instruction.
    
    Returns:
        torch.Tensor: Tensor containing only the generated tokens.
    """
    start_indices = find_sequence_start(tensor, end_of_instruction)
    
    if tensor.dim() == 1:
        # Single sequence case
        if start_indices == -1:
            raise ValueError("End of instruction sequence not found in tensor")
        return tensor[start_indices:]

    elif tensor.dim() == 2:
        # Batch of sequences case
        generated_tokens_batch = []
        for seq, start_index in zip(tensor, start_indices):
            if start_index == -1:
                raise ValueError("End of instruction sequence not found in one of the sequences in the batch")
            generated_tokens_batch.append(seq[start_index:])
        return torch.nn.utils.rnn.pad_sequence(generated_tokens_batch, batch_first=True, padding_value=pad_token_id)
    

def split_tensor_on_eos(tokenized_tensor, eos_token_ids):
    """
    Splits a tensor of tokenized text into a list of tensors for each sentence.
    
    Parameters:
    - tokenized_tensor: A tensor of shape (seq_len) or (batch_size, seq_len) containing tokenized text.
    - eos_token_ids: A list of token IDs that represent end-of-sentence tokens.
    
    Returns:
    - If input is (seq_len), returns a list of tensors, each representing a sentence.
    - If input is (batch_size, seq_len), returns a list of lists of tensors. Each inner list contains tensors representing sentences from one sequence.
    """
    if tokenized_tensor.dim() == 1:
        # Handle single sequence tensor
        return [_split_single_sequence(tokenized_tensor, eos_token_ids)]
    elif tokenized_tensor.dim() == 2:
        # Handle batch of sequences tensor
        batch_size, seq_len = tokenized_tensor.shape
        all_sentences = []

        for i in range(batch_size):
            sequence = tokenized_tensor[i]
            sentence_tensors = _split_single_sequence(sequence, eos_token_ids)
            all_sentences.append(sentence_tensors)
        
        return all_sentences
    else:
        raise ValueError("tokenized_tensor should be either of shape (seq_len) or (batch_size, seq_len)")

def _split_single_sequence(sequence, eos_token_ids):
    """
    Splits a single sequence tensor into a list of tensors for each sentence.
    
    Parameters:
    - sequence: A tensor of shape (seq_len) containing tokenized text.
    - eos_token_ids: A list of token IDs that represent end-of-sentence tokens.
    
    Returns:
    - A list of tensors, each representing a sentence.
    """
    eos_indices = [idx for idx, token_id in enumerate(sequence) if token_id in eos_token_ids]

    if not eos_indices:
        return [sequence]

    sentence_tensors = []
    start_idx = 0

    for eos_idx in eos_indices:
        sentence_tensors.append(sequence[start_idx:eos_idx + 1])
        start_idx = eos_idx + 1

    if start_idx < len(sequence):
        sentence_tensors.append(sequence[start_idx:])

    return sentence_tensors


def calculate_average_uncertainties(split_sentences, predictive_entropy, mutual_information, aleatoric_uncertainty):
    """
    Calculate the average predictive entropy, mutual information, and aleatoric uncertainty for each sentence.

    Parameters:
    split_sentences (list of lists): A list where each element is a list of sentence tensors for a batch.
    predictive_entropy (torch.Tensor): A tensor containing predictive entropy values for each token in the batch.
    mutual_information (torch.Tensor): A tensor containing mutual information values for each token in the batch.
    aleatoric_uncertainty (torch.Tensor): A tensor containing aleatoric uncertainty values for each token in the batch.

    Returns:
    tuple: A tuple containing three lists:
        - batch_pred: A list of lists, where each inner list contains average predictive entropy values for each sentence in the batch.
        - batch_MI: A list of lists, where each inner list contains average mutual information values for each sentence in the batch.
        - batch_alea: A list of lists, where each inner list contains average aleatoric uncertainty values for each sentence in the batch.
    """
    batch_pred = []
    batch_MI = []
    batch_alea = []
    for batch_idx, sentence_tensors in enumerate(split_sentences):
        pred_per_sentence = []
        mi_per_sentence = []
        alea_per_sentence = []
        sentence_start_idx = 0  # Initialize starting index for the first sentence
        for sentence_idx, sentence_tensor in enumerate(sentence_tensors):
            num_tokens = len(sentence_tensor)
            sentence_end_idx = sentence_start_idx + num_tokens  # Calculate end index for the current sentence
            
            # Extract the uncertainties for the current sentence
            pred_values = predictive_entropy[batch_idx, sentence_start_idx:sentence_end_idx]
            mi_values = mutual_information[batch_idx, sentence_start_idx:sentence_end_idx]
            alea_values = aleatoric_uncertainty[batch_idx, sentence_start_idx:sentence_end_idx]
            
            # Calculate the average uncertainties for the current sentence
            avg_pred = pred_values.mean().item()
            avg_MI = mi_values.mean().item()
            avg_alea = alea_values.mean().item()

            # Append the average sentence uncertainties to the corresponding lists
            pred_per_sentence.append(avg_pred)
            mi_per_sentence.append(avg_MI)
            alea_per_sentence.append(avg_alea)
            
            # Update the starting index for the next sentence
            sentence_start_idx = sentence_end_idx
        
        # Append the average uncertainties for the current batch to the corresponding lists
        batch_pred.append(pred_per_sentence)
        batch_MI.append(mi_per_sentence)
        batch_alea.append(alea_per_sentence)
    

    return batch_pred, batch_MI, batch_alea